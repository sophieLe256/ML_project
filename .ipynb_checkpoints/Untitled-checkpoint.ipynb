{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3f3b4f-3fde-4cbb-b399-239a8bdcbb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cb19cb-9774-4173-b003-b0a42d77cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Data and Handle Missing Values\n",
    "def load_data_with_missing_values(file_path):\n",
    "    data = pd.read_excel(file_path, header=None)\n",
    "    \n",
    "    # Handle columns containing comma-separated strings\n",
    "    expanded_data = []\n",
    "    for col in data.columns:\n",
    "        expanded_col = data[col].apply(lambda x: pd.Series(map(float, x.split(','))) if isinstance(x, str) else x)\n",
    "        expanded_data.append(expanded_col)\n",
    "\n",
    "    # Combine columns into a full DataFrame\n",
    "    data = pd.concat(expanded_data, axis=1)\n",
    "\n",
    "    # Replace missing values (1.00000000000000e+99) with NaN\n",
    "    data.replace(1.00000000000000e+99, np.nan, inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b150f2dc-7bae-4ade-bad2-80f48f5499f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Impute Missing Values\n",
    "def impute_missing_values(data, method='mean'):\n",
    "    if method == 'mean':\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "    elif method == 'median':\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "    elif method == 'knn':\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose 'mean', 'median', or 'knn'.\")\n",
    "\n",
    "    imputed_data = pd.DataFrame(imputer.fit_transform(data))\n",
    "    return imputed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86311dc-a284-4d1b-bd0e-b70d425fd9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train a Classifier\n",
    "def train_classifier(train_data, train_labels):\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    model.fit(train_data, train_labels.values.ravel())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af214af7-6edf-482c-a84c-dbc3d85d2c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Predict Test Labels\n",
    "def predict(model, test_data):\n",
    "    return model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64757fcc-fc8e-440d-807b-aa8bd8585ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Save Predictions\n",
    "def save_predictions(predictions, output_file):\n",
    "    pd.DataFrame(predictions).to_csv(output_file, index=False, header=False)\n",
    "    print(f\"Predictions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58738ea7-ca83-4b31-b5df-7bd3308f5e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Save Imputed Data\n",
    "def save_imputed_data(imputed_data, output_file):\n",
    "    imputed_data.to_excel(output_file, index=False, header=False)\n",
    "    print(f\"Imputed data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a1fc2b2-4d0d-43cb-a375-c3391e917a60",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_data_with_missing_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Skip missing value estimation for datasets > 3\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m---> 12\u001b[0m     data_with_missing_values \u001b[38;5;241m=\u001b[39m \u001b[43mload_data_with_missing_values\u001b[49m(input_file)\n\u001b[1;32m     13\u001b[0m     imputed_data \u001b[38;5;241m=\u001b[39m impute_missing_values(data_with_missing_values, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m     save_imputed_data(imputed_data, imputed_output_file)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_data_with_missing_values' is not defined"
     ]
    }
   ],
   "source": [
    "# Main Workflow\n",
    "# for i in range(1, 7):  # Loop for datasets 1-6\n",
    "#     input_file = f'./Dataset/output_MissingData{i}.xlsx'\n",
    "#     train_file = f'./Dataset/output_TrainData{i}.xlsx'\n",
    "#     label_file = f'./Dataset/output_TrainLabel{i}.xlsx'\n",
    "#     test_file = f'./Dataset/output_TestData{i}.xlsx'\n",
    "#     imputed_output_file = f'./MissingData/BleMissingResult{i}.xlsx'\n",
    "#     predictions_output_file = f'./Classification/BleClassification{i}.txt'\n",
    "\n",
    "#     # Skip missing value estimation for datasets > 3\n",
    "#     if i <= 3:\n",
    "#         data_with_missing_values = load_data_with_missing_values(input_file)\n",
    "#         imputed_data = impute_missing_values(data_with_missing_values, method='mean')\n",
    "#         save_imputed_data(imputed_data, imputed_output_file)\n",
    "\n",
    "#     train_data = load_data_with_missing_values(train_file)\n",
    "#     test_data = load_data_with_missing_values(test_file)\n",
    "#     train_labels = pd.read_excel(label_file, header=None)\n",
    "\n",
    "#     imputed_train_data = impute_missing_values(train_data, method='mean')\n",
    "#     imputed_test_data = impute_missing_values(test_data, method='mean')\n",
    "\n",
    "#     model = train_classifier(imputed_train_data, train_labels)\n",
    "#     predictions = predict(model, imputed_test_data)\n",
    "\n",
    "#     save_predictions(predictions, predictions_output_file)\n",
    "\n",
    "# Main Execution\n",
    "# Load and impute missing data\n",
    "data_with_missing_values = load_data_with_missing_values(input_file)\n",
    "imputed_data = impute_missing_values(data_with_missing_values)\n",
    "save_imputed_data(imputed_data, imputed_output_file)\n",
    "\n",
    "# Load training data\n",
    "train_data, train_labels = load_training_data(train_file, label_file)\n",
    "\n",
    "# Train the model\n",
    "model = train_classifier(train_data, train_labels)\n",
    "\n",
    "# Make predictions\n",
    "predictions = predict(model, test_file)\n",
    "\n",
    "# Save predictions\n",
    "save_predictions(predictions, predictions_output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e2d1e9-1256-4745-8140-2739f70e1bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86b3503-0547-41b6-af52-481266e604b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
